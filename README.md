# Document Search Project (Vietnamese News)

A high-performance document search engine designed to crawl, process, and index Vietnamese articles from VNExpress. This project utilizes **Sentence Embeddings** and **FAISS** (Facebook AI Similarity Search) to provide semantic search capabilities, accessible via a Streamlit web interface or a REST API.

---

## ğŸš€ Features

- **Web Crawling:** Automated extraction of articles from VNExpress by category.
- **Text Preprocessing:** Cleaning and chunking Vietnamese text for optimal indexing.
- **Semantic Search:** Uses `sentence-transformers` to understand query intent beyond simple keywords.
- **Vector Database:** High-speed similarity search powered by FAISS.
- **Interactive UI:** A user-friendly Streamlit dashboard for real-time searching.
- **Extensible API:** Flask-based backend for easy integration with other services.

---

## ğŸ“‚ Project Structure

```text
document-search-project/
â”œâ”€ crawl/
â”‚  â””â”€ crawl_vnexpress.py   
â”œâ”€ data/
â”‚  â”œâ”€ crawl/               
â”‚  â””â”€ processed/           
â”œâ”€ preprocess/
â”‚  â””â”€ clean_text.py      
â”œâ”€ search/
â”‚  â”œâ”€ build_index.py       
â”‚  â””â”€ search.py            
â”œâ”€ api/
â”‚  â””â”€ app.py              
â”œâ”€ app/
<<<<<<< HEAD
â”‚  â””â”€ streamlit_app.py     # Streamlit web application
â”œâ”€ requirements.txt        # Project dependencies
â””â”€ README.md               # Project documentation
=======
â”‚  â””â”€ streamlit_app.py     
â”œâ”€ requirements.txt        
â””â”€ README.md              
>>>>>>> aa8bef137a4a62cc51ce738468a1e76cd9db2b92
```

ğŸ› ï¸ Installation
Clone the repository:

```bash
git clone <your-repo-url>
cd document-search-project
```

Set up a Virtual Environment:

```bash
python -m venv venv
```
### Windows
```bash
venv\Scripts\activate
```
### Linux/Mac
```bash
source venv/bin/activate
```
#### Install Dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“– Usage Guide
1. Crawl Data
Extract the latest news from VNExpress. You can modify the CATEGORY_URL within the script.
```bash
python crawl/crawl_vnexpress.py
```
2. Preprocess Text
Clean the raw HTML/text and split documents into manageable chunks.
```bash
python preprocess/clean_text.py
```
3. Build Vector Index
Generate the FAISS index. This step converts text chunks into high-dimensional vectors.
```bash
python search/build_index.py
```
4. Search Documents
```bash
python search/search.py
```
#### Via Web Interface:
```bash
streamlit run app/streamlit_app.py
```
## ğŸ› ï¸ Tech Stack
Data: Requests, BeautifulSoup4, Pandas

NLP: Sentence-Transformers (Vietnamese-supported models)

Vector Engine: FAISS (CPU)

Frontend: Streamlit

Backend: Flask (Optional)

## ğŸ“ Notes

Ensure the data/crawl/ directory exists before running the crawler.

The search quality depends heavily on the embedding model used in build_index.py.

This project can be easily upgraded to a RAG (Retrieval-Augmented Generation) pipeline by connecting it to an LLM like GPT-4 or Gemini.

